{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "from dlcprojecthelper import DLCProjectHelper as dph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/local/data2/LIA_LIU_PONTUS/LIA_LIU/res_101_epoch_100-conv_vid-2024-11-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path, video_path = dph.get_config_and_video_paths(project_path=project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(config=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(config_path, maxiters=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.plot_trajectories(config='/local/data2/LIA_LIU_PONTUS/LIA_LIU/test_10_000_epochs-conv_vid-2024-10-28/config.yaml', videos=['/local/data2/LIA_LIU_PONTUS/LIA_LIU/test_10_000_epochs-conv_vid-2024-10-28/videos/1-1_convertedDLC_Resnet101_test_10_000_epochsOct28shuffle1_snapshot_1425_p50_labeled.mp4'], trainingsetindex=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config_path, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing videos with /local/data2/LIA_LIU_PONTUS/LIA_LIU/res_101_epoch_100-conv_vid-2024-11-06/dlc-models-pytorch/iteration-1/res_101_epoch_100Nov6-trainset90shuffle1/train/snapshot-100.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ponsi25/miniconda3/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/base.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  snapshot = torch.load(snapshot_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_converted.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    4020\n",
      "  Duration of video [s]:  134.00\n",
      "  fps:                    30.0\n",
      "  resolution:             w=1274, h=720\n",
      "\n",
      "Running pose prediction with batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4020/4020 [01:51<00:00, 36.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_convertedDLC_Resnet101_res_101_epoch_100Nov6shuffle1_snapshot_100.h5 and /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_convertedDLC_Resnet101_res_101_epoch_100Nov6shuffle1_snapshot_100_full.pickle\n",
      "Starting to analyze /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_converted.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    809\n",
      "  Duration of video [s]:  26.97\n",
      "  fps:                    30.0\n",
      "  resolution:             w=1274, h=720\n",
      "\n",
      "Running pose prediction with batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [00:22<00:00, 36.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_convertedDLC_Resnet101_res_101_epoch_100Nov6shuffle1_snapshot_100.h5 and /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_convertedDLC_Resnet101_res_101_epoch_100Nov6shuffle1_snapshot_100_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DLC_Resnet101_res_101_epoch_100Nov6shuffle1_snapshot_100'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(config_path, videos=[\"/local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_converted.mp4\",\"/local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_converted.mp4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process video: /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_converted.mp4Starting to process video: /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_converted.mp4\n",
      "\n",
      "Loading /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_converted.mp4 and data.Loading /local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_converted.mp4 and data.\n",
      "\n",
      "Duration of video [s]: 26.97, recorded with 30.0 fps!\n",
      "Duration of video [s]: 134.0, recorded with 30.0 fps!Overall # of frames: 809 with cropped frame dimensions: 1274 720\n",
      "\n",
      "Overall # of frames: 4020 with cropped frame dimensions: 1274 720Generating frames and creating video.\n",
      "\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [00:06<00:00, 128.40it/s]]\n",
      "100%|██████████| 4020/4020 [00:29<00:00, 134.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(config_path, videos=[\"/local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/GH010342_converted.mp4\",\"/local/data2/LIA_LIU_PONTUS/LIA_LIU/untrained_videos/res_101_epoch_100/RF_mapping_untracked_converted.mp4\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
